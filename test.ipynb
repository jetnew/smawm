{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605af9e6",
   "metadata": {},
   "source": [
    "# World Models\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "1. Collect 10k random episodes. `extract.py`\n",
    "2. Train VAE. `vae_train.py`\n",
    "3. Pre-process collected data using VAE. `series.py`\n",
    "4. Train MDN-RNN. `rnn_train.py`\n",
    "5. Run CMA-ES. `train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467fdc8",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed1eeb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://pytorch-lightning.readthedocs.io/en/latest/notebooks/course_UvA-DL/08-deep-autoencoders.html\n",
    "# ! pip install --quiet \"torch>=1.6, <1.9\" \"pytorch-lightning>=1.3\" \"torchvision\" \"seaborn\" \"torchmetrics>=0.3\"\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from collections import OrderedDict, deque, namedtuple\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "pl.seed_everything(42)\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           input_dim: Number of input features\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, latent_dim),\n",
    "            act_fn(),\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            act_fn(),\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           output_dim: Number of output features\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            act_fn(),\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            act_fn(),\n",
    "            nn.Linear(latent_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        latent_dim: int,\n",
    "        encoder_class: object = Encoder,\n",
    "        decoder_class: object = Decoder,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(input_dim, latent_dim)\n",
    "        self.decoder = decoder_class(input_dim, latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(2, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case)\"\"\"\n",
    "        x = batch\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1]).mean(dim=[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        \n",
    "class ObsDataset(data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8f4efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ObsDataset(np.ones((100,3), dtype=np.float32))\n",
    "train_loader = data.DataLoader(dataset, batch_size=32)\n",
    "val_loader = data.DataLoader(dataset, batch_size=32)\n",
    "\n",
    "model = Autoencoder(input_dim=3, latent_dim=16)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=500)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "425ecd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variational encoder model, used as a visual model\n",
    "for our model of the world.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\" VAE decoder \"\"\"\n",
    "    def __init__(self, output_dim=8, latent_dim=16, n_units=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_units = n_units\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_dim, n_units)\n",
    "        self.fc2 = nn.Linear(n_units, n_units)\n",
    "        self.fc3 = nn.Linear(n_units, output_dim)\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module): # pylint: disable=too-many-instance-attributes\n",
    "    \"\"\" VAE encoder \"\"\"\n",
    "    def __init__(self, input_dim=8, latent_dim=16, n_units=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_units = n_units\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, n_units)\n",
    "        self.fc2 = nn.Linear(n_units, n_units)\n",
    "        self.fc_mu = nn.Linear(n_units, latent_dim)\n",
    "        self.fc_logsigma = nn.Linear(n_units, latent_dim)\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        logsigma = self.fc_logsigma(x)\n",
    "        return mu, logsigma\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\" Variational Autoencoder \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.decoder = Decoder(input_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logsigma = self.encoder(x)\n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = eps.mul(sigma).add_(mu)\n",
    "\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "325426a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(np.ones((28,8), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3289676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.7793e-01, -1.1581e-01,  9.9639e-02,  1.2234e-01,  2.7219e-01,\n",
       "           1.6652e-01,  3.9213e-02,  1.3524e-02],\n",
       "         [ 1.5171e-01,  1.0672e-01,  8.0299e-02,  2.1089e-01,  1.6755e-01,\n",
       "           8.2226e-02, -3.6395e-02, -2.8142e-02],\n",
       "         [ 3.1949e-01,  1.5323e-01,  5.4087e-02,  6.0500e-02,  2.6304e-01,\n",
       "           6.5669e-02, -6.9476e-03,  3.0243e-02],\n",
       "         [ 1.4787e-01, -5.6834e-02,  2.2940e-01,  1.7896e-01,  1.5475e-01,\n",
       "           1.3417e-02,  4.5627e-02,  1.4061e-01],\n",
       "         [ 1.0682e-01,  2.5448e-02,  4.2714e-02,  1.1468e-02,  1.8435e-01,\n",
       "          -2.0532e-03,  9.6457e-03,  9.7366e-02],\n",
       "         [ 1.6481e-01,  1.3478e-01,  1.2075e-01,  2.0632e-01,  1.7861e-01,\n",
       "           3.3075e-02,  7.4786e-03,  7.9657e-02],\n",
       "         [ 1.4285e-01, -9.6923e-02,  1.5459e-01,  9.4515e-02,  2.7128e-01,\n",
       "           5.1347e-02,  1.1681e-01,  3.0045e-02],\n",
       "         [ 8.5337e-02, -1.0595e-02,  8.1042e-02,  2.1737e-01,  1.5248e-01,\n",
       "           6.2465e-02,  2.3276e-02,  1.2906e-01],\n",
       "         [ 3.3424e-02,  1.2010e-01,  7.3038e-02,  9.8695e-02,  2.8132e-01,\n",
       "           8.3044e-02,  6.3976e-02,  5.0064e-02],\n",
       "         [ 1.5320e-01, -4.8570e-02,  1.2055e-01,  1.0874e-01,  9.5001e-02,\n",
       "           1.3216e-01,  5.1337e-02, -2.7931e-02],\n",
       "         [ 2.3862e-01, -1.1633e-02,  1.3285e-01,  8.3897e-02,  1.8644e-01,\n",
       "           4.7800e-02, -1.8171e-02,  3.6315e-02],\n",
       "         [ 1.6809e-01, -8.7202e-02,  1.5514e-01,  1.8307e-01,  1.5404e-01,\n",
       "           4.8470e-03,  4.0976e-03,  1.0528e-01],\n",
       "         [ 6.4330e-02,  5.5418e-02,  1.0932e-01,  5.5370e-02,  1.9785e-01,\n",
       "          -4.8576e-02,  7.9769e-02,  5.1258e-02],\n",
       "         [ 8.6333e-02,  1.4707e-02,  1.6970e-01,  1.0374e-01,  1.8581e-01,\n",
       "           7.8041e-02,  7.0776e-02, -5.6195e-02],\n",
       "         [ 5.8605e-02,  4.8099e-02,  1.5400e-01,  1.4188e-01,  1.8393e-01,\n",
       "           9.7281e-02,  3.8286e-02, -1.6690e-02],\n",
       "         [ 8.6421e-04, -3.3852e-02,  2.1343e-01,  9.7451e-02,  1.8846e-01,\n",
       "           9.3960e-02,  3.1759e-03, -1.3798e-02],\n",
       "         [ 8.1268e-02, -2.6436e-02,  1.2951e-01,  1.1386e-01,  1.2762e-01,\n",
       "           9.6189e-02,  4.2743e-02, -2.1249e-05],\n",
       "         [ 1.3547e-01, -1.0280e-01,  1.2661e-01,  1.1923e-01,  2.1930e-01,\n",
       "           3.6300e-02,  3.6616e-02,  6.6423e-02],\n",
       "         [ 8.1217e-02,  9.0426e-02,  4.0799e-02,  6.4831e-02,  1.5858e-01,\n",
       "           6.7970e-02,  7.7205e-02,  3.4146e-02],\n",
       "         [ 1.8515e-01,  9.1115e-03,  8.5082e-02,  6.1543e-02,  2.5703e-01,\n",
       "           7.0311e-02,  5.9095e-02,  5.4884e-02],\n",
       "         [ 1.8450e-01,  6.8940e-02,  1.4000e-01,  1.0411e-01,  1.6320e-01,\n",
       "           8.0769e-02,  3.7083e-02, -4.6067e-02],\n",
       "         [ 2.1987e-01,  2.4820e-02,  2.2418e-01,  1.5193e-01,  1.6358e-01,\n",
       "           6.7401e-02,  6.4351e-02,  5.4963e-02],\n",
       "         [ 1.6326e-01, -6.5595e-02,  1.9042e-01,  1.4157e-01,  2.7963e-01,\n",
       "           1.0870e-01,  6.8597e-02,  4.0751e-02],\n",
       "         [ 2.0154e-01,  5.2853e-02,  1.5574e-01,  2.1030e-01,  7.8415e-02,\n",
       "           8.3600e-02,  5.8639e-02, -1.6750e-02],\n",
       "         [ 1.0743e-01, -5.1891e-02,  1.4298e-01,  1.3184e-01,  2.3053e-01,\n",
       "           6.9970e-02,  4.1363e-02,  1.8487e-02],\n",
       "         [ 2.5666e-01, -2.4588e-02,  1.4700e-01,  6.5775e-02,  1.8273e-01,\n",
       "           2.3553e-02,  6.8354e-02, -1.9663e-02],\n",
       "         [ 5.1899e-02,  4.0215e-02,  1.1175e-01,  1.7333e-01,  1.2749e-01,\n",
       "           9.1791e-02, -5.8474e-02,  8.9982e-02],\n",
       "         [ 2.0457e-01,  9.2459e-02,  8.8390e-02,  2.5365e-01,  1.7878e-01,\n",
       "           8.6301e-02, -2.9779e-02,  3.3038e-02]], grad_fn=<AddmmBackward>),\n",
       " tensor([[-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427],\n",
       "         [-0.1273, -0.0796, -0.1571,  0.0884,  0.1138,  0.0163, -0.0377,  0.0390,\n",
       "          -0.0032,  0.1153,  0.2785,  0.0654,  0.0040,  0.1810, -0.1322,  0.0427]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725],\n",
       "         [-0.0437, -0.1679, -0.2066,  0.0897,  0.0505, -0.1488,  0.0681,  0.1053,\n",
       "           0.0161, -0.0556, -0.0140, -0.0509, -0.0855, -0.1182, -0.1140, -0.0725]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE(8, 16)\n",
    "vae(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4dede5",
   "metadata": {},
   "source": [
    "# MDN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ed8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smawm)",
   "language": "python",
   "name": "smawm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
